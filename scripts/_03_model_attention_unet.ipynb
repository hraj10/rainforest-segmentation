{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b8c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Preprocess import train_dataset, val_dataset, test_dataset, amazon_training_image_paths, amazon_validation_image_paths, amazon_test_image_paths, batches\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, UpSampling2D, BatchNormalization, Activation, LeakyReLU, Dropout, Input, Lambda, Add, Conv2DTranspose, Concatenate, Reshape, Permute, Multiply, GlobalAveragePooling2D, Dense, Flatten\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "import tensorflow.keras.backend as K\n",
    "#from hyperparameters import alpha\n",
    "K.set_image_data_format('channels_last')\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e910656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jacobeliason/Documents/Files/School/grad/ST456 Deep Learning/group-project/project-2023-group-11/scripts/_02c_read_datasets.py:14: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 07:11:57.294165: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-27 07:11:57.294519: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import _02c_read_datasets\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# -------------------- load data\n",
    "\n",
    "augment = False\n",
    "train_dataset, val_dataset, test_dataset = _02c_read_datasets.load_datasets(augmented = augment)\n",
    "\n",
    "# ----------- create directories\n",
    "\n",
    "out_dir = '../results/' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '_UNET_ATTN/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    os.makedirs(out_dir + '/plots')\n",
    "    os.makedirs(out_dir + '/weights')\n",
    "    os.makedirs(out_dir + '/predictions')\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=out_dir+'weights/'+'model.{epoch:02d}-{val_loss:.4f}.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max', \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39dd8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor,\n",
    "                 n_filters,\n",
    "                 kernel_size=3,\n",
    "                 batchnorm=True,\n",
    "                 strides=1,\n",
    "                 dilation_rate=1,\n",
    "                 recurrent=1):\n",
    "\n",
    "    # A wrapper of the Keras Conv2D block to serve as a building block for downsampling layers\n",
    "    # Includes options to use batch normalization, dilation and recurrence\n",
    "\n",
    "    conv = Conv2D(filters=n_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  padding=\"same\",\n",
    "                  dilation_rate=dilation_rate)(input_tensor)\n",
    "    if batchnorm:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    output = LeakyReLU(alpha=0.3)(conv)\n",
    "\n",
    "    for _ in range(recurrent - 1):\n",
    "        conv = Conv2D(filters=n_filters,\n",
    "                      kernel_size=kernel_size,\n",
    "                      strides=1,\n",
    "                      kernel_initializer=\"he_normal\",\n",
    "                      padding=\"same\",\n",
    "                      dilation_rate=dilation_rate)(output)\n",
    "        if batchnorm:\n",
    "            conv = BatchNormalization()(conv)\n",
    "        res = LeakyReLU(alpha=0.3)(conv)\n",
    "        output = Add()([output, res])\n",
    "\n",
    "    return output\n",
    "\n",
    "def transpose_block(input_tensor,\n",
    "                    skip_tensor,\n",
    "                    n_filters,\n",
    "                    kernel_size=3,\n",
    "                    strides=1,\n",
    "                    batchnorm=True,\n",
    "                    recurrent=1):\n",
    "\n",
    "    # A wrapper of the Keras Conv2DTranspose block to serve as a building block for upsampling layers\n",
    "\n",
    "    shape_x = K.int_shape(input_tensor)\n",
    "    shape_xskip = K.int_shape(skip_tensor)\n",
    "\n",
    "    conv = Conv2DTranspose(filters=n_filters,\n",
    "                           kernel_size=kernel_size,\n",
    "                           padding='same',\n",
    "                           strides=(shape_xskip[1] // shape_x[1],\n",
    "                                    shape_xskip[2] // shape_x[2]),\n",
    "                           kernel_initializer=\"he_normal\")(input_tensor)\n",
    "    conv = LeakyReLU(alpha=0.3)(conv)\n",
    "\n",
    "    act = conv2d_block(conv,\n",
    "                       n_filters=n_filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=1,\n",
    "                       batchnorm=batchnorm,\n",
    "                       dilation_rate=1,\n",
    "                       recurrent=recurrent)\n",
    "    output = Concatenate(axis=3)([act, skip_tensor])\n",
    "    return output\n",
    "\n",
    "def expend_as(tensor, rep):\n",
    "\n",
    "    # Anonymous lambda function to expand the specified axis by a factor of argument, rep.\n",
    "    # If tensor has shape (512,512,N), lambda will return a tensor of shape (512,512,N*rep), if specified axis=2\n",
    "\n",
    "    my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                       arguments={'repnum': rep})(tensor)\n",
    "    return my_repeat\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape):\n",
    "\n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(g)\n",
    "\n",
    "    # Getting the gating signal to the same number of filters as the inter_shape\n",
    "    phi_g = Conv2D(filters=inter_shape,\n",
    "                   kernel_size=1,\n",
    "                   strides=1,\n",
    "                   padding='same')(g)\n",
    "\n",
    "    # Getting the x signal to the same shape as the gating signal\n",
    "    theta_x = Conv2D(filters=inter_shape,\n",
    "                     kernel_size=3,\n",
    "                     strides=(shape_x[1] // shape_g[1],\n",
    "                              shape_x[2] // shape_g[2]),\n",
    "                     padding='same')(x)\n",
    "\n",
    "    # Element-wise addition of the gating and x signals\n",
    "    # add_xg = Add([phi_g, theta_x])\n",
    "    add_xg = Add()([phi_g, theta_x])\n",
    "\n",
    "    add_xg = Activation('relu')(add_xg)\n",
    "\n",
    "    # 1x1x1 convolution\n",
    "    psi = Conv2D(filters=1, kernel_size=1, padding='same')(add_xg)\n",
    "    psi = Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(psi)\n",
    "\n",
    "    # Upsampling psi back to the original dimensions of x signal\n",
    "    upsample_sigmoid_xg = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1],\n",
    "                                             shape_x[2] //\n",
    "                                             shape_sigmoid[2]))(psi)\n",
    "\n",
    "    # Expanding the filter axis to the number of filters in the original x signal\n",
    "    upsample_sigmoid_xg = expend_as(upsample_sigmoid_xg, shape_x[3])\n",
    "\n",
    "    # Element-wise multiplication of attention coefficients back onto original x signal\n",
    "    attn_coefficients = Multiply()([upsample_sigmoid_xg, x])\n",
    "\n",
    "    # Final 1x1x1 convolution to consolidate attention signal to original x dimensions\n",
    "    output = Conv2D(filters=shape_x[3],\n",
    "                    kernel_size=1,\n",
    "                    strides=1,\n",
    "                    padding='same')(attn_coefficients)\n",
    "    output = BatchNormalization()(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def GatingSignal(input_tensor, batchnorm=True):\n",
    "\n",
    "    # 1x1x1 convolution to consolidate gating signal into the required dimensions\n",
    "    # Not required most of the time, unless another ReLU and batch_norm is required on gating signal\n",
    "\n",
    "    shape = K.int_shape(input_tensor)\n",
    "    conv = Conv2D(filters=shape[3],\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    output = LeakyReLU(alpha=alpha)(conv)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ebad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_U_network(input_shape, n_filters=16, batchnorm=True):\n",
    "\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # encoder\n",
    "    \n",
    "    c0 = conv2d_block(inputs,\n",
    "                 n_filters=n_filters,\n",
    "                 kernel_size=3,\n",
    "                 batchnorm=batchnorm,\n",
    "                 strides=1,\n",
    "                 dilation_rate=1,\n",
    "                 recurrent=2) # 256x256x256\n",
    "    \n",
    "    p0 = MaxPooling2D((2,2))(c0)\n",
    "    p0 = Dropout(0.4)(p0)\n",
    "    \n",
    "    c1 = conv2d_block(p0,\n",
    "                 n_filters=n_filters * 2,\n",
    "                 kernel_size=3,\n",
    "                 batchnorm=batchnorm,\n",
    "                 strides=1,\n",
    "                 dilation_rate=1,\n",
    "                 recurrent=2) # 128x128x128\n",
    "    \n",
    "    p1 = MaxPooling2D((2,2))(c1)\n",
    "    p1 = Dropout(0.4)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1,\n",
    "                 n_filters=n_filters * 4,\n",
    "                 kernel_size=3,\n",
    "                 batchnorm=batchnorm,\n",
    "                 strides=1,\n",
    "                 dilation_rate=1,\n",
    "                 recurrent=2) # 64x64x64\n",
    "    \n",
    "    p2 = MaxPooling2D((2,2))(c2)\n",
    "    p2 = Dropout(0.4)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2,\n",
    "                 n_filters=n_filters * 8,\n",
    "                 kernel_size=3,\n",
    "                 batchnorm=batchnorm,\n",
    "                 strides=1,\n",
    "                 dilation_rate=1,\n",
    "                 recurrent=2) # 32x32x32\n",
    "    \n",
    "    p3 = MaxPooling2D((2,2))(c3)\n",
    "    p3 = Dropout(0.4)(p3)\n",
    "\n",
    "    # bridge\n",
    "    \n",
    "    b0 = conv2d_block(p3,\n",
    "                 n_filters=n_filters * 16,\n",
    "                 kernel_size=3,\n",
    "                 batchnorm=batchnorm,\n",
    "                 strides=1,\n",
    "                 dilation_rate=1,\n",
    "                 recurrent=2) # 16x16x16\n",
    "\n",
    "    # decoder\n",
    "\n",
    "    attn0 = AttnGatingBlock(c3, b0, n_filters * 16)\n",
    "    u0 = transpose_block(b0,\n",
    "                         attn0,\n",
    "                         n_filters=n_filters * 8,\n",
    "                         batchnorm=batchnorm,\n",
    "                         recurrent=2)  # 32x32x32\n",
    "\n",
    "    attn1 = AttnGatingBlock(c2, u0, n_filters * 8)\n",
    "    u1 = transpose_block(u0,\n",
    "                         attn1,\n",
    "                         n_filters=n_filters * 4,\n",
    "                         batchnorm=batchnorm,\n",
    "                         recurrent=2)  # 64x64x64\n",
    "\n",
    "    attn2 = AttnGatingBlock(c1, u1, n_filters * 4)\n",
    "    u2 = transpose_block(u1,\n",
    "                         attn2,\n",
    "                         n_filters=n_filters * 2,\n",
    "                         batchnorm=batchnorm,\n",
    "                         recurrent=2)  # 128x128x128\n",
    "\n",
    "    u3 = transpose_block(u2,\n",
    "                         c0,\n",
    "                         n_filters=n_filters,\n",
    "                         batchnorm=batchnorm,\n",
    "                         recurrent=2)  # 256x256x256\n",
    "\n",
    "    outputs = Conv2D(filters=1, kernel_size=1, strides=1,\n",
    "                     activation='sigmoid')(u3)\n",
    "    model = Model(inputs=inputs, outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bf654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 07:12:02.980527: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-04-27 07:12:04.161505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27/125 [=====>........................] - ETA: 36s - loss: 0.2758 - accuracy: 0.8831"
     ]
    }
   ],
   "source": [
    "input_shape = (256, 256, 4)\n",
    "model = Attention_U_network(input_shape, batchnorm=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "epochs = 20\n",
    "history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- save results\n",
    "\n",
    "# load best model\n",
    "model.load_weights(out_dir + 'weights/'+'_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c92d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training and validation loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.savefig(out_dir + '/plots/' + 'loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e478740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(out_dir + '/plots/' + 'accuracy.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save weights\n",
    "# model.save(out_dir + '/weights/' + 'model.h5')\n",
    "\n",
    "# save predictions\n",
    "def visualize_predictions(index, test_dataset, out_dir, batches = 16):\n",
    "    \n",
    "    dir = \"image_\" + str(index)\n",
    "    if not os.path.exists(out_dir + '/predictions/' + dir + '/'):\n",
    "        os.makedirs(out_dir + '/predictions/' + dir + '/')\n",
    "        os.makedirs(out_dir + '/predictions/' + dir + '/input_image')\n",
    "        os.makedirs(out_dir + '/predictions/' + dir + '/ground_truth')\n",
    "        os.makedirs(out_dir + '/predictions/' + dir + '/prediction')\n",
    "        os.makedirs(out_dir + '/predictions/' + dir + '/prediction_binary')\n",
    "    \n",
    "    test_data_iter = iter(itertools.cycle(test_dataset))\n",
    "\n",
    "    for i in range(index + 1):\n",
    "        image_batch, label_batch = next(test_data_iter)\n",
    "\n",
    "    wrapped_index = index % 16\n",
    "    image = image_batch[wrapped_index].numpy()\n",
    "    image_rgb = np.stack(\n",
    "        (\n",
    "            (image[:,:,0] - np.min(image[:,:,0])) * 255.0 / (np.max(image[:,:,0]) - np.min(image[:,:,0])),\n",
    "            (image[:,:,1] - np.min(image[:,:,1])) * 255.0 / (np.max(image[:,:,1]) - np.min(image[:,:,1])),\n",
    "            (image[:,:,2] - np.min(image[:,:,2])) * 255.0 / (np.max(image[:,:,2]) - np.min(image[:,:,2]))\n",
    "        ),\n",
    "        axis=-1\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    prediction = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "    plt.imsave(out_dir + '/predictions/' + dir + '/input_image/' + str(index) + '.png', image_rgb)\n",
    "\n",
    "    ground_truth = label_batch[wrapped_index].numpy()\n",
    "    plt.imsave(out_dir + '/predictions/' + dir + '/ground_truth/' + str(index) + '.png', np.squeeze(ground_truth), cmap='gray')\n",
    "\n",
    "    plt.imsave(out_dir + '/predictions/' + dir + '/prediction/' + str(index) + '.png', np.squeeze(prediction), cmap='gray')\n",
    "\n",
    "    prediction_binary = np.where(prediction > 0.5, 1, 0)\n",
    "    plt.imsave(out_dir + '/predictions/' + dir + '/prediction_binary/' + str(index) + '.png', np.squeeze(prediction_binary), cmap='gray')\n",
    "\n",
    "for i in range(80):\n",
    "    visualize_predictions(i, test_dataset, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- save metrics\n",
    "\n",
    "if augment:\n",
    "    augmetation_settings = {\n",
    "    \"flip_left_right\": 0,\n",
    "    \"flip_up_down\": 0,\n",
    "    \"gaussian_blur\": 0.2,\n",
    "    \"random_noise\": 0.0,\n",
    "    \"random_brightness\": 0.5,\n",
    "    \"random_contrast\": 0.5}\n",
    "else:\n",
    "    augmetation_settings = None\n",
    "\n",
    "batches = 16\n",
    "shuffled = True\n",
    "\n",
    "model_info = _02_evaluate_model.evaluate_model(\n",
    "    \"U-net without attention; final dataset\", \n",
    "    test_dataset,\n",
    "    model, \n",
    "    input_shape, \n",
    "    shuffled, \n",
    "    batches, \n",
    "    epochs, \n",
    "    augmentation_settings=augmetation_settings, \n",
    "    threshold=0.5\n",
    "    )\n",
    "df = pd.DataFrame(model_info)\n",
    "df.to_csv(os.path.join(out_dir, 'metrics.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
